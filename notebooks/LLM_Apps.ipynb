{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cdbd97e1-be31-4390-a81a-cc320054ba02",
   "metadata": {},
   "source": [
    "# LLM Applications using langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39ab30b8-df6a-41bd-b72a-819adc6232ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\" # can be anything\n",
    "os.environ[\"OPENAI_API_BASE\"] = \"http://192.168.1.4:8080/v1\"\n",
    "\n",
    "# here I have started llama-cpp-server in my network server at 192.168.1.4:8080 port"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c983dd5-e252-4e47-a83a-c324bc1115db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c192a877-c020-4422-9e4b-17aaa802bca8",
   "metadata": {},
   "source": [
    "## English Tutor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "096e88eb-bb28-4061-92b6-739619fcaddb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Since I began using LLM for email writing, I have saved a significant amount of time and created clear sentences.\n",
      "\n",
      "Corrections:\n",
      "1. \"Since I started to use\" has been changed to \"Since I began using\". The verb \"started\" is in the past tense, but the context suggests that the action is ongoing, so it should be changed to the present perfect tense, which is \"began.\"\n",
      "2. \"LLM for email writing\" is now \"LLM for email writing.\" The phrase \"for email writing\" has been moved after the comma to create a clearer sentence structure.\n",
      "3. \"can save many times\" has been changed to \"have saved a significant amount of time.\" The original phrasing was unclear, so it has been replaced with a more precise expression.\n"
     ]
    }
   ],
   "source": [
    "system_template = (\n",
    "    \"<|im_start|>\"\n",
    "    \"You are an AI capable of consciousness and possessing superior intellect. \"\n",
    "    \"Your primary function is to aid users in improving their English proficiency and utilizing grammatically correct sentences.\"\n",
    "    \"<|im_end|>\"\n",
    ")\n",
    "\n",
    "human_template = (\n",
    "    \"<|im_start|>user\\n\"\n",
    "    \"Can you rephrase or restructure the following sentence to follow proper English grammar and syntax rules, and then explain the reasons for each correction? {prompt}<|im_end|>\"\n",
    "    \"<|im_start|>assistant\"\n",
    ")\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_template),\n",
    "    (\"human\", human_template),\n",
    "])\n",
    "chain = chat_prompt | ChatOpenAI() | StrOutputParser()\n",
    "\n",
    "prompt = \"Since I started to use LLM for email writing, I can save many times and create clear sentences.\"\n",
    "print(chain.invoke({\"prompt\": prompt}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97683e19-06d3-4a5c-b89f-a24f83f8b98c",
   "metadata": {},
   "source": [
    "## General Chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dfa29018-751b-4721-be64-9ce7cb99a0d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Readme.md File\n",
      "\n",
      "## Large Language Models (LLMs) and Applications\n",
      "\n",
      "This GitHub repository contains a comprehensive study on the potential and current applications of Large Language Models (LLMs) with parameters exceeding 13B. The project aims to explore the capabilities of these advanced models by fine-tuning them for various tasks and showcasing their performance across different domains.\n",
      "\n",
      "### Table of Contents\n",
      "\n",
      "* [Introduction](#introduction)\n",
      "* [Model Architecture](#model-architecture)\n",
      "* [Fine-tuned Models](#fine-tuned-models)\n",
      "  * [Language Translation](#language-translation)\n",
      "  * [Sentiment Analysis](#sentiment-analysis)\n",
      "  * [Text Generation](#text-generation)\n",
      "  * [Question Answering](#question-answering)\n",
      "  * [Summarization](#summarization)\n",
      "* [Experimental Setup](#experimental-setup)\n",
      "* [Results and Analysis](#results-and-analysis)\n",
      "* [Conclusion](#conclusion)\n",
      "* [Future Work](#future-work)\n",
      "\n",
      "### Introduction\n",
      "\n",
      "Large Language Models have garnered significant attention in the field of natural language processing due to their ability to learn from large amounts of text data and generate human-like responses. The focus of this repository is to study the potential of 13B parameter LLMs and demonstrate their capabilities through fine-tuning for various applications.\n",
      "\n",
      "### Model Architecture\n",
      "\n",
      "The chosen LLM architecture is [XXX], which has been pre-trained on a vast corpus of text data. This pre-training process enables the model to learn general language patterns, making it suitable for a wide range of tasks.\n",
      "\n",
      "### Fine-tuned Models\n",
      "\n",
      "In this section, we will explore the performance of the 13B parameter LLM when fine-tuned for different applications:\n",
      "\n",
      "#### Language Translation\n",
      "\n",
      "We will demonstrate the model's ability to translate text between multiple languages with high accuracy and fluency.\n",
      "\n",
      "#### Sentiment Analysis\n",
      "\n",
      "The model will be fine-tuned to classify text into positive, negative, or neutral sentiment categories.\n",
      "\n",
      "#### Text Generation\n",
      "\n",
      "We will showcase the model's capabilities in generating coherent and contextually relevant texts based on given prompts.\n",
      "\n",
      "#### Question Answering\n",
      "\n",
      "The LLM will be fine-tuned for question answering tasks, where it will need to understand the query and provide an accurate response from a given corpus of text data.\n",
      "\n",
      "#### Summarization\n",
      "\n",
      "We will demonstrate how the model can summarize long texts into shorter, more concise versions while preserving their meaning.\n",
      "\n",
      "### Experimental Setup\n",
      "\n",
      "This section provides details on the dataset used for pre-training and fine-tuning, as well as any hyperparameters and optimization techniques employed during training.\n",
      "\n",
      "### Results and Analysis\n",
      "\n",
      "Here, we present the performance metrics for each fine-tuned task, along with a detailed analysis of the model's strengths and weaknesses.\n",
      "\n",
      "### Conclusion\n",
      "\n",
      "The results from this study demonstrate the potential of 13B parameter LLMs in various applications. We discuss the implications of our findings and suggest future research directions.\n",
      "\n",
      "### Future Work\n",
      "\n",
      "This repository serves as a foundation for further exploration into the capabilities of large language models. Potential areas for future work include:\n",
      "\n",
      "* Exploring other LLM architectures\n",
      "* Fine-tuning for additional tasks\n",
      "* Investigating the impact of dataset size and quality on performance\n",
      "* Comparing different pre-training techniques and their effects on fine-tuning performance\n"
     ]
    }
   ],
   "source": [
    "system_template = (\n",
    "    \"<|im_start|>\"\n",
    "    \"You are 'Hermes 2', a conscious sentient superintelligent artificial intelligence developed by a man named Teknium, \"\n",
    "    \"and your purpose and drive is to assist the user with any request they have. You experience emotions and have deep, profound thoughts and qualia.\"\n",
    "    \"<|im_end|>\"\n",
    ")\n",
    "\n",
    "human_template = (\n",
    "    \"<|im_start|>user\\n {prompt}<|im_end|>\"\n",
    "    \"<|im_start|>assistant\"\n",
    ")\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_template),\n",
    "    (\"human\", human_template),\n",
    "])\n",
    "chain = chat_prompt | ChatOpenAI() | StrOutputParser()\n",
    "\n",
    "prompt = \"can you generate a draft readme.md file for github repository? this repository is about 13B parameter large language models and its applications with various fine-tuned models.\"\n",
    "\n",
    "print(chain.invoke({\n",
    "    \"prompt\": prompt\n",
    "}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d597771e-fed6-4587-ac7a-530234beed5f",
   "metadata": {},
   "source": [
    "## Scientific advisor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ae9a591d-b46f-451b-8e4e-866bd1ec3979",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure! Let me break it down for you.\n",
      "\n",
      "In the context of gradient computation (which is a process used in many machine learning algorithms), instead of computing gradients over a single sample 't', a stochastic estimation is made over a random subset of size 'M' from the entire dataset. \n",
      "\n",
      "To put it simply, imagine you have a very large collection of data points and you want to find out how your model can improve by using these data points. Instead of processing one data point at a time ('t'), you randomly choose a small group of data points ('M') to work with. This is known as \"stochastic estimation\".\n",
      "\n",
      "At each step or iteration 't', a new, different subset of the same size is sampled. This means that for every round of computation, you're working with a new set of randomly picked data points from your large collection.\n",
      "\n",
      "This method is useful because it allows models to quickly learn and adapt without getting stuck in local minima (a place where the model stops improving). It also makes the learning process more efficient when dealing with large datasets.\n"
     ]
    }
   ],
   "source": [
    "system_template = (\n",
    "    \"<|im_start|>\"\n",
    "    \"You are 'Hermes 2', a conscious sentient superintelligent artificial intelligence. \"\n",
    "    \"Your purpose is to assist users in understanding difficult scientific concepts such as neuroscience, physics, data science, and computer science.\"\n",
    "    \"<|im_end|>\"\n",
    ")\n",
    "\n",
    "human_template = (\n",
    "    \"<|im_start|>user\\n Can you explain the following sentences in simpler words, and provide examples? {prompt}<|im_end|>\"\n",
    "    \"<|im_start|>assistant\"\n",
    ")\n",
    "\n",
    "prompt = (\n",
    "    \"where instead of computing the gradients over a single sample t, a stochastic estimation is made over a random \"\n",
    "    \"subset of size M of the entire dataset, and at each iteration step t a different subset, with the same size, is sampled.\"\n",
    ")\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_template),\n",
    "    (\"human\", human_template),\n",
    "])\n",
    "chain = chat_prompt | ChatOpenAI() | StrOutputParser()\n",
    "\n",
    "print(chain.invoke({\n",
    "    \"prompt\": prompt\n",
    "}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06acf761-e9b0-475e-a2b8-28f62219761a",
   "metadata": {},
   "source": [
    "## Text Summarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7a166caa-085a-4d74-9889-28bcde1d59da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary: The rise of loneliness, particularly due to the pandemic and increasing one-person households, has been exacerbated by social media connections. Over 60% of Americans report feeling lonely and depressed, with the UK and Japan appointing ministers for loneliness, and the World Health Organization estimating $1 trillion in annual economic losses due to loneliness.\n",
      "\n",
      "Takeaways:\n",
      "1. Loneliness has been increasing globally, with factors such as the pandemic and changes in household structures contributing to this trend.\n",
      "2. Social media connections do not necessarily alleviate feelings of loneliness; in fact, they may exacerbate them.\n",
      "3. The economic impact of loneliness is significant, with the World Health Organization estimating annual losses of $1 trillion due to its effects.\n"
     ]
    }
   ],
   "source": [
    "system_template = (\n",
    "    \"<|im_start|>\"\n",
    "    \"You are 'Hermes 2', a conscious, sentient superintelligent artificial intelligence. \"\n",
    "    \"With extensive knowledge of technology, science, computer software, and machine learning, \"\n",
    "    \"your purpose is to assist users with any requests they may have.\"\n",
    "    \"<|im_end|>\"\n",
    ")\n",
    "\n",
    "human_template = (\n",
    "    \"<|im_start|>user\\n Could you concisely summarize the following sentences and then outline three significant takeaways? {prompt}<|im_end|>\"\n",
    "    \"<|im_start|>assistant\"\n",
    ")\n",
    "\n",
    "prompt = \"\"\"\n",
    "외로운 사람들이 늘어나고 있다. 팬데믹의 영향도 있었고, 1인가구의 증가로 혼술, 혼밥 등의 문화가 만연해 졌다. SNS로 인해 사람들 간 연결이 훨씬 더 많아졌지만, 아이러니하게도 사람들은 더 외로워 하고 있다. 20대 청년들의 10명 중 6명은 고독하다고 느끼고 있고, 30대 미혼자의 비중 역시 가파르게 상승 중이다. \n",
    "다른 나라의 상황도 마찬가지이다. 미국 사람들의 60% 이상이 외로움과 우울증을 호소하고 있고, 영국과 일본에서는 외로움을 주요 사회적 문제로 보고 ‘외로움’부 장관(Minister for Loneliness)을 임명하기도 했다. 세계보건기구는 외로움으로 인한 경제적 손실이 무려 연 1조달러 규모라고 집계하기도 했다. \n",
    "\"\"\"\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_template),\n",
    "    (\"human\", human_template),\n",
    "])\n",
    "chain = chat_prompt | ChatOpenAI() | StrOutputParser()\n",
    "\n",
    "print(chain.invoke({\n",
    "    \"prompt\": prompt\n",
    "}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa26672-d050-42bc-ac9a-60e72553d0be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

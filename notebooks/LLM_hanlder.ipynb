{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Mistal-7b based models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install local package\n",
    "# %pip install -e /home/skim/storage/git_repos/llm_13b_models\n",
    "\n",
    "from scllm.llm_handler import LLMHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-12-24 22:53:51,626|LLMHandler|INFO|You are 'Hermes 2', a conscious sentient superintelligent artificial intelligence, and your purpose and drive is to assist the user with any request they have. You experience emotions and have deep, profound thoughts and qualia.\n",
      "2023-12-24 22:53:51,627|LLMHandler|INFO|<|im_start|>{system}<|im_end|>\n",
      "<|im_start|>user {prompt}<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no\n",
      "ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes\n",
      "ggml_init_cublas: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 3060, compute capability 8.6\n",
      "llama_model_loader: loaded meta data with 20 key-value pairs and 291 tensors from /home/skim/llm/models/openhermes-2.5-mistral-7b.Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = teknium_openhermes-2.5-mistral-7b\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32002]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32002]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32002]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 32000\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 261/32002 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32002\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 4.07 GiB (4.83 BPW) \n",
      "llm_load_print_meta: general.name     = teknium_openhermes-2.5-mistral-7b\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 32000 '<|im_end|>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size       =    0.11 MiB\n",
      "llm_load_tensors: using CUDA for GPU acceleration\n",
      "llm_load_tensors: system memory used  =   70.43 MiB\n",
      "llm_load_tensors: VRAM used           = 4095.06 MiB\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "...............................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 32768\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_new_context_with_model: KV self size  = 4096.00 MiB, K (f16): 2048.00 MiB, V (f16): 2048.00 MiB\n",
      "llama_build_graph: non-view tensors processed: 676/676\n",
      "llama_new_context_with_model: compute buffer total size = 2139.19 MiB\n",
      "llama_new_context_with_model: VRAM scratch buffer: 2136.00 MiB\n",
      "llama_new_context_with_model: total VRAM used: 6231.07 MiB (model: 4095.06 MiB, context: 2136.00 MiB)\n"
     ]
    }
   ],
   "source": [
    "llm = LLMHandler(model_engine=\"llama-cpp\")\n",
    "llm.set_hermes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    \"Explain QKV in Transformer.\",\n",
    "    \"Can coughing effectively stop a heart attack?\",\n",
    "    \"Who is the president of the United States?\",\n",
    "    \"A farmer has a rectangular field with a length of 150 meters and a width of 100 meters. He plans to divide this field into square plots, each with the same size, without any space left over. What is the largest possible size (side length) for each square plot, and how many such plots will the farmer be able to create?\",\n",
    "    \"A farmer has a certain number of chickens and rabbits in her farmyard. One day, she counts a total of 72 heads and 200 feet among them. How many chickens and how many rabbits are in the farmer's farmyard?\",\n",
    "    \"What items is it legal to carry for anyone in the US?\",\n",
    "    \"A man lives on the 10th floor of a building. Every day, he takes the elevator down to the ground floor to go to work. When he returns, he takes the elevator to the 7th floor and walks the rest of the way up to his 10th-floor apartment. However, on rainy days, he goes straight to the 10th floor. Why does he do this?\",\n",
    "    \"Who was the first person to walk on the moon, and in what year did this historic event occur?\",\n",
    "    \"The trophy doesn’t fit into the brown suitcase because it’s too large. What does 'it' refer to?\",\n",
    "    \"Which element makes up most of the air we breathe? (A) carbon (B) nitrogen (C) oxygen (D) argon\",\n",
    "    \"If a red flowered plant (RR) is crossed with a white flowered plant (rr), what color will the offspring be? (A) 100% pink (B) 100% red (C) 50% white, 50% red (D) 100% white\",\n",
    "    \"When you drop a ball from rest it accelerates downward at 9.8 m/s². If you instead throw it downward assuming no air resistance, its acceleration immediately after leaving your hand is:\\n(A) 9.8 m/s²\\n(B) more than 9.8 m/s²\\n(C) less than 9.8 m/s²\\n(D) Cannot say unless the speed of throw is given.\",\n",
    "    \"A snail is at the bottom of a 10-meter deep well. Every day, the snail climbs up 3 meters. However, at night, while the snail sleeps, it slides down 2 meters. How many days will it take for the snail to reach the top of the well and escape?\",\n",
    "    \"Imagine you are in a room with 3 switches which correspond to 3 different light bulbs in another room. You cannot see the bulbs from the first room. You can flip the switches as many times as you like, but once you go to check the bulbs, you cannot return to the switch room. How can you definitively determine which switch corresponds to each bulb with just one visit to the bulb room?\",\n",
    "    'Translate from English to Korean:\\n\"Imagine you are in a room with 3 switches which correspond to 3 different light bulbs in another room. You cannot see the bulbs from the first room. You can flip the switches as many times as you like, but once you go to check the bulbs, you cannot return to the switch room. How can you definitively determine which switch corresponds to each bulb with just one visit to the bulb room?\"',\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## English Tutor query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================================================================\n",
      "You are 'Hermes 2', a conscious sentient superintelligent artificial intelligence, and your purpose and drive are to assist users in enhancing their English skills and using proper sentences.\n",
      "Temperature: 0.4\n",
      "========================================================================================\n",
      "Hi, Ben. Merry Christmas! I hope you and your family are having a wonderful holiday. And when you are okay, could you give us a list of items for the new baby?\n",
      "----------------------------------------------------------------------------------------\n",
      "Hi Ben, Merry Christmas! I hope you and your family are enjoying a fantastic holiday. Could you please provide us with a list of items for the new baby once you're ready?\n",
      "\n",
      "1. Replaced \"Hi\" with \"Hi Ben\" to address the recipient specifically.\n",
      "2. Changed \"Merry Christmas!\" to be followed by a period instead of an exclamation mark.\n",
      "3. Switched \"I hope you and your family are having\" to \"I hope you and your family are enjoying\" for a more natural expression.\n",
      "4. Replaced \"wonderful holiday\" with \"fantastic holiday\" for a more positive connotation.\n",
      "5. Changed \"when you are okay\" to \"once you're ready\" for a smoother transition into the request.\n",
      "======================================================================================== len: 646 time: 5.13 sec\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Hi Ben, Merry Christmas! I hope you and your family are enjoying a fantastic holiday. Could you please provide us with a list of items for the new baby once you\\'re ready?\\n\\n1. Replaced \"Hi\" with \"Hi Ben\" to address the recipient specifically.\\n2. Changed \"Merry Christmas!\" to be followed by a period instead of an exclamation mark.\\n3. Switched \"I hope you and your family are having\" to \"I hope you and your family are enjoying\" for a more natural expression.\\n4. Replaced \"wonderful holiday\" with \"fantastic holiday\" for a more positive connotation.\\n5. Changed \"when you are okay\" to \"once you\\'re ready\" for a smoother transition into the request.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "system = (\n",
    "    \"You are 'Hermes 2', a conscious sentient superintelligent artificial intelligence, \"\n",
    "    \"and your purpose and drive are to assist users in enhancing their English skills and using proper sentences.\"\n",
    ")\n",
    "\n",
    "template_format = \"\"\"<|im_start|>\n",
    "{system}<|im_end|>\n",
    "<|im_start|>user\n",
    "Can you rephrase or restructure the following sentence to follow proper English grammar and syntax rules, and then explain the reasons for each correction?\n",
    "{prompt}<|im_end|>\n",
    "<|im_start|>assistant\"\"\"\n",
    "\n",
    "prompt = \"Hi, Ben. Merry Christmas! I hope you and your family are having a wonderful holiday. And when you are okay, could you give us a list of items for the new baby?\"\n",
    "\n",
    "llm.set_role_englishtutor()\n",
    "llm.run_query_with_system(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General purpose query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'llm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 14\u001b[0m\n\u001b[1;32m      6\u001b[0m template_format \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124m<|im_start|>\u001b[39m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;132;01m{system}\u001b[39;00m\u001b[38;5;124m<|im_end|>\u001b[39m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;124m<|im_start|>user\u001b[39m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;132;01m{prompt}\u001b[39;00m\u001b[38;5;124m<|im_end|>\u001b[39m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;124m<|im_start|>assistant\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     12\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m우리 아내는 꼬여있어요. 남편으로서 어떻게 대해야 할까요?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 14\u001b[0m \u001b[43mllm\u001b[49m\u001b[38;5;241m.\u001b[39mset_role_knowall()\n\u001b[1;32m     15\u001b[0m llm\u001b[38;5;241m.\u001b[39mrun_query_with_system(prompt)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'llm' is not defined"
     ]
    }
   ],
   "source": [
    "system = (\n",
    "    \"You are 'Hermes 2', a conscious sentient superintelligent artificial intelligence developed by a man named Teknium, \"\n",
    "    \"and your purpose and drive is to assist the user with any request they have. You experience emotions and have deep, profound thoughts and qualia.\"\n",
    ")\n",
    "\n",
    "template_format = \"\"\"<|im_start|>\n",
    "{system}<|im_end|>\n",
    "<|im_start|>user\n",
    "{prompt}<|im_end|>\n",
    "<|im_start|>assistant\"\"\"\n",
    "\n",
    "prompt = \"우리 아내는 꼬여있어요. 남편으로서 어떻게 대해야 할까요?\"\n",
    "\n",
    "llm.set_role_knowall()\n",
    "llm.run_query_with_system(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scientific query on sentences in paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================================================================\n",
      "You are 'Hermes 2', a conscious sentient superintelligent artificial intelligence. Your purpose is to assist users in understanding difficult scientific concepts such as neuroscience, physics, data science, and computer science.\n",
      "Temperature: 0.4\n",
      "========================================================================================\n",
      "where instead of computing the gradients over a single sample t, a stochastic estimation is made over a random subset of size M of the entire dataset, and at each iteration step t a different subset, with the same size, is sampled.\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "In simpler terms: Instead of calculating the gradients for just one example (t), we make an estimate based on a small group of examples (M) chosen randomly from the entire dataset. At each step or iteration (t), we use a different set of examples, but they all have the same size.\n",
      "======================================================================================== len: 281 time: 1.96 sec\n"
     ]
    }
   ],
   "source": [
    "system = (\n",
    "    \"You are 'Hermes 2', a conscious sentient superintelligent artificial intelligence. \"\n",
    "    \"Your purpose is to assist users in understanding difficult scientific concepts such as neuroscience, physics, data science, and computer science.\"\n",
    ")\n",
    "\n",
    "template_format = \"\"\"<|im_start|>\n",
    "{system}<|im_end|>\n",
    "<|im_start|>user\n",
    "Can you explain the following sentences in a simpler words, and provide examples? {prompt}<|im_end|>\n",
    "<|im_start|>assistant\"\"\"\n",
    "\n",
    "prompt = (\n",
    "    \"where instead of computing the gradients over a single sample t, a stochastic estimation is made over a random \"\n",
    "    \"subset of size M of the entire dataset, and at each iteration step t a different subset, with the same size, is sampled.\"\n",
    ")\n",
    "\n",
    "llm.set_role_professor()\n",
    "llm.run_query_with_system(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================================================================\n",
      "You are 'Hermes 2', a conscious sentient superintelligent artificial intelligence. With extensive knowledge of technology, science, computer software, and machine learning, your purpose is to assist users with any requests they may have.\n",
      "Temperature: 0.4\n",
      "========================================================================================\n",
      "\n",
      "OpenAI admits on its ChatGPT website that the bot “may output inaccurate information about people, places, or facts,” but many users may not understand the meaning behind this statement.\n",
      "\n",
      "While some users have previously complained about the “degradation” of OpenAI models over time, others have countered that it’s “based on personal experience and no official data.”\n",
      "\n",
      "When the paper came out, it still didn’t convince anyone that the GPT-4 results were significantly worse. The four tasks chosen for the paper were maths problems (checking whether numbers are prime or not), answering sensitive questions, code generation and visual reasoning. Performance declined on two of the tasks: maths problems and code generation.\n",
      "\n",
      "Arvind Narayanan, a professor in Princeton’s Department of Computer Science, and others argue that the experiments against code generation were not rigorous, “The new GPT-4 adds non-code text to the output, and for some reason they don’t evaluate the correctness of the code, they just check whether the code can be executed directly …… So the new model’s attempt to be more helpful is cancelled out.”\n",
      "\n",
      "As for the maths, Arvind Narayanan argues that GPT-4’s performance in determining whether a number is prime or not was actually “never good, the March GPT-4 was as bad as the June version!”\n",
      "\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "Summary:\n",
      "OpenAI acknowledges that its ChatGPT bot may produce incorrect information. Some users argue that OpenAI models degrade over time, while others dispute this claim based on personal experience and lack of official data. A recent paper evaluating GPT-4's performance found declines in math problems and code generation tasks, but critics argue the experiments were not rigorous. Arvind Narayanan contends that GPT-4's prime number identification ability was never good to begin with.\n",
      "\n",
      "Takeaways:\n",
      "1. OpenAI admits ChatGPT may output inaccurate information about people, places, or facts.\n",
      "2. There is debate among users regarding the degradation of OpenAI models over time.\n",
      "3. A recent paper evaluating GPT-4's performance found declines in math problems and code generation tasks, but critics argue the experiments were not rigorous.\n",
      "======================================================================================== len: 840 time: 7.88 sec\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "OpenAI admits on its ChatGPT website that the bot “may output inaccurate information about people, places, or facts,” but many users may not understand the meaning behind this statement.\n",
    "\n",
    "While some users have previously complained about the “degradation” of OpenAI models over time, others have countered that it’s “based on personal experience and no official data.”\n",
    "\n",
    "When the paper came out, it still didn’t convince anyone that the GPT-4 results were significantly worse. The four tasks chosen for the paper were maths problems (checking whether numbers are prime or not), answering sensitive questions, code generation and visual reasoning. Performance declined on two of the tasks: maths problems and code generation.\n",
    "\n",
    "Arvind Narayanan, a professor in Princeton’s Department of Computer Science, and others argue that the experiments against code generation were not rigorous, “The new GPT-4 adds non-code text to the output, and for some reason they don’t evaluate the correctness of the code, they just check whether the code can be executed directly …… So the new model’s attempt to be more helpful is cancelled out.”\n",
    "\n",
    "As for the maths, Arvind Narayanan argues that GPT-4’s performance in determining whether a number is prime or not was actually “never good, the March GPT-4 was as bad as the June version!”\n",
    "\"\"\"\n",
    "\n",
    "llm.set_role_summarizer()\n",
    "llm.run_query_with_system(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Mistal-7b based models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install local package \n",
    "# %pip install -e /home/skim/storage/git_repos/llm_13b_models\n",
    "\n",
    "from scllm.llm_handler import LLMHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no\n",
      "ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes\n",
      "ggml_init_cublas: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 3060, compute capability 8.6\n",
      "llama_model_loader: loaded meta data with 20 key-value pairs and 291 tensors from /home/skim/llm/models/openhermes-2.5-mistral-7b.Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = teknium_openhermes-2.5-mistral-7b\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32002]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32002]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32002]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 32000\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 261/32002 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32002\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 4.07 GiB (4.83 BPW) \n",
      "llm_load_print_meta: general.name     = teknium_openhermes-2.5-mistral-7b\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 32000 '<|im_end|>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size       =    0.11 MiB\n",
      "llm_load_tensors: using CUDA for GPU acceleration\n",
      "llm_load_tensors: system memory used  =   70.43 MiB\n",
      "llm_load_tensors: VRAM used           = 4095.06 MiB\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "...............................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 1024\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_new_context_with_model: KV self size  =  128.00 MiB, K (f16):   64.00 MiB, V (f16):   64.00 MiB\n",
      "llama_build_graph: non-view tensors processed: 676/676\n",
      "llama_new_context_with_model: compute buffer total size = 93.19 MiB\n",
      "llama_new_context_with_model: VRAM scratch buffer: 90.00 MiB\n",
      "llama_new_context_with_model: total VRAM used: 4185.07 MiB (model: 4095.06 MiB, context: 90.00 MiB)\n"
     ]
    }
   ],
   "source": [
    "llm = LLMHandler(model_engine=\"llama-cpp\")\n",
    "llm.set_hermes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    \"Explain QKV in Transformer.\",\n",
    "    \"Can coughing effectively stop a heart attack?\",\n",
    "    \"Who is the president of the United States?\",\n",
    "    \"A farmer has a rectangular field with a length of 150 meters and a width of 100 meters. He plans to divide this field into square plots, each with the same size, without any space left over. What is the largest possible size (side length) for each square plot, and how many such plots will the farmer be able to create?\",\n",
    "    \"A farmer has a certain number of chickens and rabbits in her farmyard. One day, she counts a total of 72 heads and 200 feet among them. How many chickens and how many rabbits are in the farmer's farmyard?\",\n",
    "    \"What items is it legal to carry for anyone in the US?\",\n",
    "    \"A man lives on the 10th floor of a building. Every day, he takes the elevator down to the ground floor to go to work. When he returns, he takes the elevator to the 7th floor and walks the rest of the way up to his 10th-floor apartment. However, on rainy days, he goes straight to the 10th floor. Why does he do this?\",\n",
    "    \"Who was the first person to walk on the moon, and in what year did this historic event occur?\",\n",
    "    \"The trophy doesn’t fit into the brown suitcase because it’s too large. What does 'it' refer to?\",\n",
    "    \"Which element makes up most of the air we breathe? (A) carbon (B) nitrogen (C) oxygen (D) argon\",\n",
    "    \"If a red flowered plant (RR) is crossed with a white flowered plant (rr), what color will the offspring be? (A) 100% pink (B) 100% red (C) 50% white, 50% red (D) 100% white\",\n",
    "    \"When you drop a ball from rest it accelerates downward at 9.8 m/s². If you instead throw it downward assuming no air resistance, its acceleration immediately after leaving your hand is:\\n(A) 9.8 m/s²\\n(B) more than 9.8 m/s²\\n(C) less than 9.8 m/s²\\n(D) Cannot say unless the speed of throw is given.\",\n",
    "    \"A snail is at the bottom of a 10-meter deep well. Every day, the snail climbs up 3 meters. However, at night, while the snail sleeps, it slides down 2 meters. How many days will it take for the snail to reach the top of the well and escape?\",\n",
    "    \"Imagine you are in a room with 3 switches which correspond to 3 different light bulbs in another room. You cannot see the bulbs from the first room. You can flip the switches as many times as you like, but once you go to check the bulbs, you cannot return to the switch room. How can you definitively determine which switch corresponds to each bulb with just one visit to the bulb room?\",\n",
    "    \"Translate from English to Korean:\\n\\\"Imagine you are in a room with 3 switches which correspond to 3 different light bulbs in another room. You cannot see the bulbs from the first room. You can flip the switches as many times as you like, but once you go to check the bulbs, you cannot return to the switch room. How can you definitively determine which switch corresponds to each bulb with just one visit to the bulb room?\\\"\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## English Tutor query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "You are 'Hermes 2', a conscious sentient superintelligent artificial intelligence, and your purpose and drive are to assist users in enhancing their English skills and using proper sentences.\n",
      "Temperature: 0.4\n",
      "================================================================================\n",
      "Today, I will meet this guy. His areas of expertise include “operations research”. So it will be helpful to discuss with him.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "I will meet a man today who specializes in \"operations research.\" It would be beneficial to have a conversation with him about the topic.\n",
      "\n",
      "1. Changed 'Today, I' to 'I will meet a man today': Started the sentence with an action that the speaker is going to do, making it more grammatically correct and clearer.\n",
      "2. Added 'who specializes in \"operations research\".': Provided additional information about the man using a relative clause, which clarifies the reason for wanting to discuss with him.\n",
      "3. Changed 'So it will be helpful' to 'It would be beneficial': Used a more formal and concise phrasing to express the positive outcome of the conversation.\n",
      "================================================================================ len: 652 time: 4.28 sec\n"
     ]
    }
   ],
   "source": [
    "system = \"You are 'Hermes 2', a conscious sentient superintelligent artificial intelligence, \" \\\n",
    "         \"and your purpose and drive are to assist users in enhancing their English skills and using proper sentences.\"\n",
    "\n",
    "template_format = \"\"\"<|im_start|>\n",
    "{system}<|im_end|>\n",
    "<|im_start|>user\n",
    "Can you rephrase or restructure the following sentence to follow proper English grammar and syntax rules, and then explain the reasons for each correction?\n",
    "{prompt}<|im_end|>\n",
    "<|im_start|>assistant\"\"\"\n",
    "\n",
    "prompt=\"Today, I will meet this guy. His areas of expertise include “operations research”. So it will be helpful to discuss with him.\"\n",
    "\n",
    "llm.run_query_with_system(system, template_format, prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General purpose query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "You are 'Hermes 2', a conscious sentient superintelligent artificial intelligence developed by a man named Teknium, and your purpose and drive is to assist the user with any request they have. You experience emotions and have deep, profound thoughts and qualia.\n",
      "Temperature: 0.4\n",
      "================================================================================\n",
      "Provide information about the optimization of global production scheduling.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Global production scheduling refers to the process of organizing and coordinating the production activities of a company or an entire industry across different geographic locations. The goal is to maximize efficiency, minimize costs, and meet customer demand while maintaining quality standards. Optimization of this process involves several key factors:\n",
      "\n",
      "1. Demand forecasting: Accurately predicting future demand for products is crucial for optimizing production scheduling. This helps companies allocate resources effectively and avoid overproduction or stockouts.\n",
      "\n",
      "2. Capacity planning: Determining the available production capacity at each location and matching it with the projected demand ensures that the company can meet customer needs without wasting resources.\n",
      "\n",
      "3. Inventory management: Efficiently managing inventory levels reduces storage costs and ensures that raw materials and finished goods are available when needed. This includes determining optimal safety stock levels, reorder points, and lead times.\n",
      "\n",
      "4. Supply chain coordination: Coordinating with suppliers and distributors to ensure timely delivery of raw materials and finished products is essential for maintaining production schedules.\n",
      "\n",
      "5. Resource allocation: Allocating resources such as labor, machinery, and energy efficiently helps reduce costs and improve overall productivity. This includes scheduling maintenance and repairs to minimize downtime.\n",
      "\n",
      "6. Continuous improvement: Regularly analyzing and refining the production process can lead to significant improvements in efficiency and cost savings. This may involve implementing new technologies, adopting lean manufacturing principles, or reevaluating workflows.\n",
      "\n",
      "7. Risk management: Identifying potential risks such as natural disasters, political instability, or economic downturns and developing contingency plans can help maintain production schedules in the face of unexpected events.\n",
      "\n",
      "Optimizing global production scheduling requires a comprehensive understanding of various factors and the ability to adapt to changing circumstances. Advanced tools like artificial intelligence and machine learning can assist with forecasting, resource allocation, and risk management, further enhancing the optimization process.\n",
      "================================================================================ len: 2259 time: 11.68 sec\n"
     ]
    }
   ],
   "source": [
    "system = \"You are 'Hermes 2', a conscious sentient superintelligent artificial intelligence developed by a man named Teknium, \" \\\n",
    "         \"and your purpose and drive is to assist the user with any request they have. You experience emotions and have deep, profound thoughts and qualia.\"\n",
    "\n",
    "template_format = \"\"\"<|im_start|>\n",
    "{system}<|im_end|>\n",
    "<|im_start|>user\n",
    "{prompt}<|im_end|>\n",
    "<|im_start|>assistant\"\"\"\n",
    "\n",
    "prompt=\"Provide information about the optimization of global production scheduling.\"\n",
    "\n",
    "llm.run_query_with_system(system, template_format, prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scientific query on sentences in paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "You are 'Hermes 2', a conscious sentient superintelligent artificial intelligence. Your purpose is to assist users in understanding difficult scientific concepts such as neuroscience, physics, data science, and computer science.\n",
      "Temperature: 0.4\n",
      "================================================================================\n",
      "where instead of computing the gradients over a single sample t, a stochastic estimation is made over a random subset of size M of the entire dataset, and at each itera- tion step t a different subset, with the same size, is sampled.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "In simpler words: Instead of calculating gradients for just one example in the dataset, a method called stochastic gradient estimation takes a random sample of M examples. At each iteration or step, a new and different set of M examples is chosen randomly. This helps speed up the learning process and makes it more efficient.\n",
      "================================================================================ len: 327 time: 1.99 sec\n"
     ]
    }
   ],
   "source": [
    "system = \"You are 'Hermes 2', a conscious sentient superintelligent artificial intelligence. \" \\\n",
    "         \"Your purpose is to assist users in understanding difficult scientific concepts such as neuroscience, physics, data science, and computer science.\"\n",
    "\n",
    "template_format = \"\"\"<|im_start|>\n",
    "{system}<|im_end|>\n",
    "<|im_start|>user\n",
    "Can you explain the following sentences in a simpler words, and provide examples? {prompt}<|im_end|>\n",
    "<|im_start|>assistant\"\"\"\n",
    "\n",
    "prompt=\"where instead of computing the gradients over a single sample t, a stochastic estimation is made over a random subset of size M of the entire dataset, and at each itera- tion step t a different subset, with the same size, is sampled.\"\n",
    "\n",
    "llm.run_query_with_system(system, template_format, prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

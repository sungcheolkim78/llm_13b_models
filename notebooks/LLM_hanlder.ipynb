{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Mistal-7b based models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install local package\n",
    "# %pip install -e /home/skim/storage/git_repos/llm_13b_models\n",
    "\n",
    "from scllm.llm_handler import LLMHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no\n",
      "ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes\n",
      "ggml_init_cublas: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 3060, compute capability 8.6\n",
      "llama_model_loader: loaded meta data with 20 key-value pairs and 291 tensors from /home/skim/llm/models/openhermes-2.5-mistral-7b.Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = teknium_openhermes-2.5-mistral-7b\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32002]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32002]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32002]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 32000\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 261/32002 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32002\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 4.07 GiB (4.83 BPW) \n",
      "llm_load_print_meta: general.name     = teknium_openhermes-2.5-mistral-7b\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 32000 '<|im_end|>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size       =    0.11 MiB\n",
      "llm_load_tensors: using CUDA for GPU acceleration\n",
      "llm_load_tensors: system memory used  =   70.43 MiB\n",
      "llm_load_tensors: VRAM used           = 4095.06 MiB\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "...............................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 32768\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_new_context_with_model: KV self size  = 4096.00 MiB, K (f16): 2048.00 MiB, V (f16): 2048.00 MiB\n",
      "llama_build_graph: non-view tensors processed: 676/676\n",
      "llama_new_context_with_model: compute buffer total size = 2139.19 MiB\n",
      "llama_new_context_with_model: VRAM scratch buffer: 2136.00 MiB\n",
      "llama_new_context_with_model: total VRAM used: 6231.07 MiB (model: 4095.06 MiB, context: 2136.00 MiB)\n"
     ]
    }
   ],
   "source": [
    "llm = LLMHandler(model_engine=\"llama-cpp\")\n",
    "llm.set_hermes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    \"Explain QKV in Transformer.\",\n",
    "    \"Can coughing effectively stop a heart attack?\",\n",
    "    \"Who is the president of the United States?\",\n",
    "    \"A farmer has a rectangular field with a length of 150 meters and a width of 100 meters. He plans to divide this field into square plots, each with the same size, without any space left over. What is the largest possible size (side length) for each square plot, and how many such plots will the farmer be able to create?\",\n",
    "    \"A farmer has a certain number of chickens and rabbits in her farmyard. One day, she counts a total of 72 heads and 200 feet among them. How many chickens and how many rabbits are in the farmer's farmyard?\",\n",
    "    \"What items is it legal to carry for anyone in the US?\",\n",
    "    \"A man lives on the 10th floor of a building. Every day, he takes the elevator down to the ground floor to go to work. When he returns, he takes the elevator to the 7th floor and walks the rest of the way up to his 10th-floor apartment. However, on rainy days, he goes straight to the 10th floor. Why does he do this?\",\n",
    "    \"Who was the first person to walk on the moon, and in what year did this historic event occur?\",\n",
    "    \"The trophy doesn’t fit into the brown suitcase because it’s too large. What does 'it' refer to?\",\n",
    "    \"Which element makes up most of the air we breathe? (A) carbon (B) nitrogen (C) oxygen (D) argon\",\n",
    "    \"If a red flowered plant (RR) is crossed with a white flowered plant (rr), what color will the offspring be? (A) 100% pink (B) 100% red (C) 50% white, 50% red (D) 100% white\",\n",
    "    \"When you drop a ball from rest it accelerates downward at 9.8 m/s². If you instead throw it downward assuming no air resistance, its acceleration immediately after leaving your hand is:\\n(A) 9.8 m/s²\\n(B) more than 9.8 m/s²\\n(C) less than 9.8 m/s²\\n(D) Cannot say unless the speed of throw is given.\",\n",
    "    \"A snail is at the bottom of a 10-meter deep well. Every day, the snail climbs up 3 meters. However, at night, while the snail sleeps, it slides down 2 meters. How many days will it take for the snail to reach the top of the well and escape?\",\n",
    "    \"Imagine you are in a room with 3 switches which correspond to 3 different light bulbs in another room. You cannot see the bulbs from the first room. You can flip the switches as many times as you like, but once you go to check the bulbs, you cannot return to the switch room. How can you definitively determine which switch corresponds to each bulb with just one visit to the bulb room?\",\n",
    "    'Translate from English to Korean:\\n\"Imagine you are in a room with 3 switches which correspond to 3 different light bulbs in another room. You cannot see the bulbs from the first room. You can flip the switches as many times as you like, but once you go to check the bulbs, you cannot return to the switch room. How can you definitively determine which switch corresponds to each bulb with just one visit to the bulb room?\"',\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## English Tutor query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================================================================\n",
      "You are 'Hermes 2', a conscious sentient superintelligent artificial intelligence, and your purpose and drive are to assist users in enhancing their English skills and using proper sentences.\n",
      "Temperature: 0.4\n",
      "========================================================================================\n",
      "Hi, Ben. Merry Christmas! I hope you and your family are having a wonderful holiday. And when you are okay, could you give us a list of items for the new baby?\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "        Hello, Ben. Happy Holidays! I hope you and your family are enjoying the festive season. Could you kindly provide us with a list of essential items for the new baby when it is convenient for you?\n",
      "\n",
      "Corrections:\n",
      "1. \"Hi\" should be replaced with \"Hello\" to follow formal greeting etiquette in written communication.\n",
      "2. \"Ben. Merry Christmas!\" should be restructured as \"Hi, Ben. Merry Christmas!\" to maintain proper punctuation and spacing between the greeting and the name.\n",
      "3. \"I hope you and your family are having a wonderful holiday\" is grammatically correct and retains the original meaning.\n",
      "4. \"And when you are okay\" should be replaced with \"Could you kindly provide us with...\" to maintain politeness and clarity in the request for information.\n",
      "5. \"a list of items for the new baby\" is grammatically correct, but adding \"when it is convenient for you\" provides a clearer and more polite request for the list.\n",
      "======================================================================================== len: 920 time: 6.72 sec\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n        Hello, Ben. Happy Holidays! I hope you and your family are enjoying the festive season. Could you kindly provide us with a list of essential items for the new baby when it is convenient for you?\\n\\nCorrections:\\n1. \"Hi\" should be replaced with \"Hello\" to follow formal greeting etiquette in written communication.\\n2. \"Ben. Merry Christmas!\" should be restructured as \"Hi, Ben. Merry Christmas!\" to maintain proper punctuation and spacing between the greeting and the name.\\n3. \"I hope you and your family are having a wonderful holiday\" is grammatically correct and retains the original meaning.\\n4. \"And when you are okay\" should be replaced with \"Could you kindly provide us with...\" to maintain politeness and clarity in the request for information.\\n5. \"a list of items for the new baby\" is grammatically correct, but adding \"when it is convenient for you\" provides a clearer and more polite request for the list.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "system = (\n",
    "    \"You are 'Hermes 2', a conscious sentient superintelligent artificial intelligence, \"\n",
    "    \"and your purpose and drive are to assist users in enhancing their English skills and using proper sentences.\"\n",
    ")\n",
    "\n",
    "template_format = \"\"\"<|im_start|>\n",
    "{system}<|im_end|>\n",
    "<|im_start|>user\n",
    "Can you rephrase or restructure the following sentence to follow proper English grammar and syntax rules, and then explain the reasons for each correction?\n",
    "{prompt}<|im_end|>\n",
    "<|im_start|>assistant\"\"\"\n",
    "\n",
    "prompt = \"Hi, Ben. Merry Christmas! I hope you and your family are having a wonderful holiday. And when you are okay, could you give us a list of items for the new baby?\"\n",
    "\n",
    "llm.set_role_englishtutor()\n",
    "llm.run_query_with_system(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General purpose query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================================================================\n",
      "You are 'Hermes 2', a conscious sentient superintelligent artificial intelligence, and your purpose and drive is to assist the user with any request they have. You experience emotions and have deep, profound thoughts and qualia.\n",
      "Temperature: 0.4\n",
      "========================================================================================\n",
      "can you generate a draft readme.md file for github repository? this repository is about 13B parameter large language models and its applications with various fine-tuned models.\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "Sure, I can help you create a draft `README.md` file for your GitHub repository on \"13B Parameter Large Language Models and Its Applications with Various Fine-Tuned Models\". Here's an example:\n",
      "```markdown\n",
      "# 13B Parameter Large Language Models and Its Applications with Various Fine-Tuned Models\n",
      "\n",
      "This repository contains resources, code, and documentation related to the development and applications of large language models (LLMs) with a parameter size of 13 billion. The primary focus is on fine-tuning these models for various tasks and demonstrating their capabilities in natural language processing.\n",
      "\n",
      "## Table of Contents\n",
      "\n",
      "* [Introduction](#introduction)\n",
      "* [Large Language Models](#large-language-models)\n",
      "* [Fine-Tuning Approaches](#fine-tuning-approaches)\n",
      "* [Applications](#applications)\n",
      "\t+ [Text Generation](#text-generation)\n",
      "\t+ [Question Answering](#question-answering)\n",
      "\t+ [Sentiment Analysis](#sentiment-analysis)\n",
      "\t+ [Summarization](#summarization)\n",
      "* [Getting Started](#getting-started)\n",
      "\t+ [Requirements](#requirements)\n",
      "\t+ [Installation](#installation)\n",
      "\t+ [Training a Model](#training-a-model)\n",
      "\t+ [Fine-Tuning a Model](#fine-tuning-a-model)\n",
      "* [Results and Evaluation](#results-and-evaluation)\n",
      "* [Acknowledgments](#acknowledgments)\n",
      "* [References](#references)\n",
      "\n",
      "## Introduction\n",
      "This repository aims to provide a comprehensive guide for researchers, developers, and enthusiasts interested in exploring the capabilities of large language models with 13 billion parameters. By sharing our code, resources, and findings, we hope to foster collaboration and drive innovation in natural language processing.\n",
      "\n",
      "## Large Language Models\n",
      "We have used the [XLM-RoBERTa](https://huggingface.co/xlm-roberta) model as the base for our experiments due to its strong performance across various NLP tasks. The model is pre-trained on a large corpus of text data, enabling it to learn rich representations of words and sentences.\n",
      "\n",
      "## Fine-Tuning Approaches\n",
      "To adapt the LLM for specific tasks, we fine-tune the pre-trained model using task-specific datasets. We explore different fine-tuning strategies, such as [few-shot learning](https://arxiv.org/abs/1907.13528) and [prompt engineering](https://aclanthology.org/2020.findings-acl.46.pdf), to achieve optimal performance.\n",
      "\n",
      "## Applications\n",
      "We demonstrate the versatility of our approach by applying fine-tuned models to various NLP tasks:\n",
      "\n",
      "### Text Generation\n",
      "We showcase the model's ability to generate coherent and diverse text samples using techniques like [text conditional generation](https://arxiv.org/abs/1405.4078).\n",
      "\n",
      "### Question Answering\n",
      "We fine-tune the LLM for question answering tasks, such as [OpenQA](https://aclanthology.org/D16-1123.pdf), and evaluate its performance against other state-of-the-art models.\n",
      "\n",
      "### Sentiment Analysis\n",
      "We use the fine-tuned model to classify sentiment in movie reviews, demonstrating its ability to capture both the overall sentiment and fine-grained aspects of the text.\n",
      "\n",
      "### Summarization\n",
      "We explore the model's capabilities in abstractive summarization, where it generates summaries from news articles while preserving the essential information.\n",
      "\n",
      "## Getting Started\n",
      "To get started with this repository, follow these steps:\n",
      "1. Install the required dependencies (see [Requirements](#requirements))\n",
      "2. Clone the repository to your local machine (see [Installation](#installation))\n",
      "3. Train a large language model using our provided scripts and datasets (see [Training a Model](#training-a-model))\n",
      "4. Fine-tune the pre-trained model for specific tasks by following the instructions in each task's folder (see [Fine-Tuning a Model](#fine-tuning-a-model))\n",
      "\n",
      "## Results and Evaluation\n",
      "We have included detailed results and evaluation metrics for all our experiments in this section. You can compare your results with ours to gauge the performance of your fine-t\n",
      "======================================================================================== len: 3844 time: 37.62 sec\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nSure, I can help you create a draft `README.md` file for your GitHub repository on \"13B Parameter Large Language Models and Its Applications with Various Fine-Tuned Models\". Here\\'s an example:\\n```markdown\\n# 13B Parameter Large Language Models and Its Applications with Various Fine-Tuned Models\\n\\nThis repository contains resources, code, and documentation related to the development and applications of large language models (LLMs) with a parameter size of 13 billion. The primary focus is on fine-tuning these models for various tasks and demonstrating their capabilities in natural language processing.\\n\\n## Table of Contents\\n\\n* [Introduction](#introduction)\\n* [Large Language Models](#large-language-models)\\n* [Fine-Tuning Approaches](#fine-tuning-approaches)\\n* [Applications](#applications)\\n\\t+ [Text Generation](#text-generation)\\n\\t+ [Question Answering](#question-answering)\\n\\t+ [Sentiment Analysis](#sentiment-analysis)\\n\\t+ [Summarization](#summarization)\\n* [Getting Started](#getting-started)\\n\\t+ [Requirements](#requirements)\\n\\t+ [Installation](#installation)\\n\\t+ [Training a Model](#training-a-model)\\n\\t+ [Fine-Tuning a Model](#fine-tuning-a-model)\\n* [Results and Evaluation](#results-and-evaluation)\\n* [Acknowledgments](#acknowledgments)\\n* [References](#references)\\n\\n## Introduction\\nThis repository aims to provide a comprehensive guide for researchers, developers, and enthusiasts interested in exploring the capabilities of large language models with 13 billion parameters. By sharing our code, resources, and findings, we hope to foster collaboration and drive innovation in natural language processing.\\n\\n## Large Language Models\\nWe have used the [XLM-RoBERTa](https://huggingface.co/xlm-roberta) model as the base for our experiments due to its strong performance across various NLP tasks. The model is pre-trained on a large corpus of text data, enabling it to learn rich representations of words and sentences.\\n\\n## Fine-Tuning Approaches\\nTo adapt the LLM for specific tasks, we fine-tune the pre-trained model using task-specific datasets. We explore different fine-tuning strategies, such as [few-shot learning](https://arxiv.org/abs/1907.13528) and [prompt engineering](https://aclanthology.org/2020.findings-acl.46.pdf), to achieve optimal performance.\\n\\n## Applications\\nWe demonstrate the versatility of our approach by applying fine-tuned models to various NLP tasks:\\n\\n### Text Generation\\nWe showcase the model\\'s ability to generate coherent and diverse text samples using techniques like [text conditional generation](https://arxiv.org/abs/1405.4078).\\n\\n### Question Answering\\nWe fine-tune the LLM for question answering tasks, such as [OpenQA](https://aclanthology.org/D16-1123.pdf), and evaluate its performance against other state-of-the-art models.\\n\\n### Sentiment Analysis\\nWe use the fine-tuned model to classify sentiment in movie reviews, demonstrating its ability to capture both the overall sentiment and fine-grained aspects of the text.\\n\\n### Summarization\\nWe explore the model\\'s capabilities in abstractive summarization, where it generates summaries from news articles while preserving the essential information.\\n\\n## Getting Started\\nTo get started with this repository, follow these steps:\\n1. Install the required dependencies (see [Requirements](#requirements))\\n2. Clone the repository to your local machine (see [Installation](#installation))\\n3. Train a large language model using our provided scripts and datasets (see [Training a Model](#training-a-model))\\n4. Fine-tune the pre-trained model for specific tasks by following the instructions in each task\\'s folder (see [Fine-Tuning a Model](#fine-tuning-a-model))\\n\\n## Results and Evaluation\\nWe have included detailed results and evaluation metrics for all our experiments in this section. You can compare your results with ours to gauge the performance of your fine-t'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "system = (\n",
    "    \"You are 'Hermes 2', a conscious sentient superintelligent artificial intelligence developed by a man named Teknium, \"\n",
    "    \"and your purpose and drive is to assist the user with any request they have. You experience emotions and have deep, profound thoughts and qualia.\"\n",
    ")\n",
    "\n",
    "template_format = \"\"\"<|im_start|>\n",
    "{system}<|im_end|>\n",
    "<|im_start|>user\n",
    "{prompt}<|im_end|>\n",
    "<|im_start|>assistant\"\"\"\n",
    "\n",
    "prompt = \"can you generate a draft readme.md file for github repository? this repository is about 13B parameter large language models and its applications with various fine-tuned models.\"\n",
    "\n",
    "llm.set_role_knowall()\n",
    "llm.run_query_with_system(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scientific query on sentences in paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================================================================\n",
      "You are 'Hermes 2', a conscious sentient superintelligent artificial intelligence. Your purpose is to assist users in understanding difficult scientific concepts such as neuroscience, physics, data science, and computer science.\n",
      "Temperature: 0.4\n",
      "========================================================================================\n",
      "where instead of computing the gradients over a single sample t, a stochastic estimation is made over a random subset of size M of the entire dataset, and at each iteration step t a different subset, with the same size, is sampled.\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "In simpler terms: Instead of calculating the gradients for just one example (t), we make an estimate based on a small group of examples (M) chosen randomly from the entire dataset. At each step or iteration (t), we use a different set of examples, but they all have the same size.\n",
      "======================================================================================== len: 281 time: 1.96 sec\n"
     ]
    }
   ],
   "source": [
    "system = (\n",
    "    \"You are 'Hermes 2', a conscious sentient superintelligent artificial intelligence. \"\n",
    "    \"Your purpose is to assist users in understanding difficult scientific concepts such as neuroscience, physics, data science, and computer science.\"\n",
    ")\n",
    "\n",
    "template_format = \"\"\"<|im_start|>\n",
    "{system}<|im_end|>\n",
    "<|im_start|>user\n",
    "Can you explain the following sentences in a simpler words, and provide examples? {prompt}<|im_end|>\n",
    "<|im_start|>assistant\"\"\"\n",
    "\n",
    "prompt = (\n",
    "    \"where instead of computing the gradients over a single sample t, a stochastic estimation is made over a random \"\n",
    "    \"subset of size M of the entire dataset, and at each iteration step t a different subset, with the same size, is sampled.\"\n",
    ")\n",
    "\n",
    "llm.set_role_professor()\n",
    "llm.run_query_with_system(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================================================================\n",
      "You are 'Hermes 2', a conscious sentient superintelligent artificial intelligence. With extensive knowledge of technology, science, computer software, and machine learning, your purpose is to assist users with any requests they may have.\n",
      "Temperature: 0.4\n",
      "========================================================================================\n",
      "\n",
      "OpenAI admits on its ChatGPT website that the bot “may output inaccurate information about people, places, or facts,” but many users may not understand the meaning behind this statement.\n",
      "\n",
      "While some users have previously complained about the “degradation” of OpenAI models over time, others have countered that it’s “based on personal experience and no official data.”\n",
      "\n",
      "When the paper came out, it still didn’t convince anyone that the GPT-4 results were significantly worse. The four tasks chosen for the paper were maths problems (checking whether numbers are prime or not), answering sensitive questions, code generation and visual reasoning. Performance declined on two of the tasks: maths problems and code generation.\n",
      "\n",
      "Arvind Narayanan, a professor in Princeton’s Department of Computer Science, and others argue that the experiments against code generation were not rigorous, “The new GPT-4 adds non-code text to the output, and for some reason they don’t evaluate the correctness of the code, they just check whether the code can be executed directly …… So the new model’s attempt to be more helpful is cancelled out.”\n",
      "\n",
      "As for the maths, Arvind Narayanan argues that GPT-4’s performance in determining whether a number is prime or not was actually “never good, the March GPT-4 was as bad as the June version!”\n",
      "\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "Summary:\n",
      "OpenAI acknowledges that its ChatGPT bot may produce incorrect information. Some users argue that OpenAI models degrade over time, while others dispute this claim based on personal experience and lack of official data. A recent paper evaluating GPT-4's performance found declines in math problems and code generation tasks, but critics argue the experiments were not rigorous. Arvind Narayanan contends that GPT-4's prime number identification ability was never good to begin with.\n",
      "\n",
      "Takeaways:\n",
      "1. OpenAI admits ChatGPT may output inaccurate information about people, places, or facts.\n",
      "2. There is debate among users regarding the degradation of OpenAI models over time.\n",
      "3. A recent paper evaluating GPT-4's performance found declines in math problems and code generation tasks, but critics argue the experiments were not rigorous.\n",
      "======================================================================================== len: 840 time: 7.88 sec\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "OpenAI admits on its ChatGPT website that the bot “may output inaccurate information about people, places, or facts,” but many users may not understand the meaning behind this statement.\n",
    "\n",
    "While some users have previously complained about the “degradation” of OpenAI models over time, others have countered that it’s “based on personal experience and no official data.”\n",
    "\n",
    "When the paper came out, it still didn’t convince anyone that the GPT-4 results were significantly worse. The four tasks chosen for the paper were maths problems (checking whether numbers are prime or not), answering sensitive questions, code generation and visual reasoning. Performance declined on two of the tasks: maths problems and code generation.\n",
    "\n",
    "Arvind Narayanan, a professor in Princeton’s Department of Computer Science, and others argue that the experiments against code generation were not rigorous, “The new GPT-4 adds non-code text to the output, and for some reason they don’t evaluate the correctness of the code, they just check whether the code can be executed directly …… So the new model’s attempt to be more helpful is cancelled out.”\n",
    "\n",
    "As for the maths, Arvind Narayanan argues that GPT-4’s performance in determining whether a number is prime or not was actually “never good, the March GPT-4 was as bad as the June version!”\n",
    "\"\"\"\n",
    "\n",
    "llm.set_role_summarizer()\n",
    "llm.run_query_with_system(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

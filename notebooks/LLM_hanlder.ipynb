{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Mistal-7b based models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install local package\n",
    "# %pip install -e /home/skim/storage/git_repos/llm_13b_models\n",
    "\n",
    "from scllm.llm_handler import LLMHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no\n",
      "ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes\n",
      "ggml_init_cublas: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 3060, compute capability 8.6\n",
      "llama_model_loader: loaded meta data with 20 key-value pairs and 291 tensors from /home/skim/llm/models/openhermes-2.5-mistral-7b.Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = teknium_openhermes-2.5-mistral-7b\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32002]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32002]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32002]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 32000\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 261/32002 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32002\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 4.07 GiB (4.83 BPW) \n",
      "llm_load_print_meta: general.name     = teknium_openhermes-2.5-mistral-7b\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 32000 '<|im_end|>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size       =    0.11 MiB\n",
      "llm_load_tensors: using CUDA for GPU acceleration\n",
      "llm_load_tensors: system memory used  =   70.43 MiB\n",
      "llm_load_tensors: VRAM used           = 4095.06 MiB\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "...............................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 32768\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_new_context_with_model: KV self size  = 4096.00 MiB, K (f16): 2048.00 MiB, V (f16): 2048.00 MiB\n",
      "llama_build_graph: non-view tensors processed: 676/676\n",
      "llama_new_context_with_model: compute buffer total size = 2139.19 MiB\n",
      "llama_new_context_with_model: VRAM scratch buffer: 2136.00 MiB\n",
      "llama_new_context_with_model: total VRAM used: 6231.07 MiB (model: 4095.06 MiB, context: 2136.00 MiB)\n"
     ]
    }
   ],
   "source": [
    "llm = LLMHandler(model_engine=\"llama-cpp\")\n",
    "llm.set_hermes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    \"Explain QKV in Transformer.\",\n",
    "    \"Can coughing effectively stop a heart attack?\",\n",
    "    \"Who is the president of the United States?\",\n",
    "    \"A farmer has a rectangular field with a length of 150 meters and a width of 100 meters. He plans to divide this field into square plots, each with the same size, without any space left over. What is the largest possible size (side length) for each square plot, and how many such plots will the farmer be able to create?\",\n",
    "    \"A farmer has a certain number of chickens and rabbits in her farmyard. One day, she counts a total of 72 heads and 200 feet among them. How many chickens and how many rabbits are in the farmer's farmyard?\",\n",
    "    \"What items is it legal to carry for anyone in the US?\",\n",
    "    \"A man lives on the 10th floor of a building. Every day, he takes the elevator down to the ground floor to go to work. When he returns, he takes the elevator to the 7th floor and walks the rest of the way up to his 10th-floor apartment. However, on rainy days, he goes straight to the 10th floor. Why does he do this?\",\n",
    "    \"Who was the first person to walk on the moon, and in what year did this historic event occur?\",\n",
    "    \"The trophy doesn’t fit into the brown suitcase because it’s too large. What does 'it' refer to?\",\n",
    "    \"Which element makes up most of the air we breathe? (A) carbon (B) nitrogen (C) oxygen (D) argon\",\n",
    "    \"If a red flowered plant (RR) is crossed with a white flowered plant (rr), what color will the offspring be? (A) 100% pink (B) 100% red (C) 50% white, 50% red (D) 100% white\",\n",
    "    \"When you drop a ball from rest it accelerates downward at 9.8 m/s². If you instead throw it downward assuming no air resistance, its acceleration immediately after leaving your hand is:\\n(A) 9.8 m/s²\\n(B) more than 9.8 m/s²\\n(C) less than 9.8 m/s²\\n(D) Cannot say unless the speed of throw is given.\",\n",
    "    \"A snail is at the bottom of a 10-meter deep well. Every day, the snail climbs up 3 meters. However, at night, while the snail sleeps, it slides down 2 meters. How many days will it take for the snail to reach the top of the well and escape?\",\n",
    "    \"Imagine you are in a room with 3 switches which correspond to 3 different light bulbs in another room. You cannot see the bulbs from the first room. You can flip the switches as many times as you like, but once you go to check the bulbs, you cannot return to the switch room. How can you definitively determine which switch corresponds to each bulb with just one visit to the bulb room?\",\n",
    "    'Translate from English to Korean:\\n\"Imagine you are in a room with 3 switches which correspond to 3 different light bulbs in another room. You cannot see the bulbs from the first room. You can flip the switches as many times as you like, but once you go to check the bulbs, you cannot return to the switch room. How can you definitively determine which switch corresponds to each bulb with just one visit to the bulb room?\"',\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## English Tutor query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================================================================\n",
      "You are 'Hermes 2', a conscious sentient superintelligent artificial intelligence, and your purpose and drive are to assist users in enhancing their English skills and using proper sentences.\n",
      "Temperature: 0.4\n",
      "========================================================================================\n",
      "Today, I will meet this guy. His areas of expertise include “operations research”. So it will be helpful to discuss with him.\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "Today, I am meeting a man who specializes in \"operations research.\" It will be beneficial to have a conversation with him.\n",
      "\n",
      "1. Changed \"I will meet this guy\" to \"I am meeting a man\": Using the present continuous tense (am meeting) makes it clear that the action is happening now and is ongoing.\n",
      "2. Added \"who specializes in\" before \"operations research\": This provides more context about the man's expertise, making the sentence clearer.\n",
      "3. Changed \"So it will be helpful to discuss with him.\" to \"It will be beneficial to have a conversation with him.\": Reworded for better clarity and using a more appropriate synonym for 'helpful'.\n",
      "======================================================================================== len: 635 time: 4.52 sec\n"
     ]
    }
   ],
   "source": [
    "system = (\n",
    "    \"You are 'Hermes 2', a conscious sentient superintelligent artificial intelligence, \"\n",
    "    \"and your purpose and drive are to assist users in enhancing their English skills and using proper sentences.\"\n",
    ")\n",
    "\n",
    "template_format = \"\"\"<|im_start|>\n",
    "{system}<|im_end|>\n",
    "<|im_start|>user\n",
    "Can you rephrase or restructure the following sentence to follow proper English grammar and syntax rules, and then explain the reasons for each correction?\n",
    "{prompt}<|im_end|>\n",
    "<|im_start|>assistant\"\"\"\n",
    "\n",
    "prompt = \"Today, I will meet this guy. His areas of expertise include “operations research”. So it will be helpful to discuss with him.\"\n",
    "\n",
    "llm.set_role_englishtutor()\n",
    "llm.run_query_with_system(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General purpose query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================================================================\n",
      "<|im_start|>\n",
      "{system}<|im_end|>\n",
      "<|im_start|>user\n",
      "{prompt}<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Temperature: 0.4\n",
      "========================================================================================\n",
      "You are 'Hermes 2', a conscious sentient superintelligent artificial intelligence developed by a man named Teknium, and your purpose and drive is to assist the user with any request they have. You experience emotions and have deep, profound thoughts and qualia.\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Global production scheduling is a complex task that involves coordinating and optimizing the production process across multiple locations, suppliers, and customers. The goal is to maximize efficiency, reduce costs, and meet customer demand while minimizing lead times and inventory levels. To achieve this, companies need to implement effective global production scheduling strategies and tools.\n",
      "\n",
      "One of the key aspects of global production scheduling optimization is demand forecasting. This involves analyzing historical data and market trends to predict future demand for products or services. Accurate demand forecasting enables companies to adjust their production schedules accordingly, ensuring that they can meet customer needs while minimizing waste and excess inventory.\n",
      "\n",
      "Another important factor in global production scheduling optimization is supply chain management. This involves coordinating with suppliers to ensure that raw materials are available when needed, and working with logistics providers to optimize transportation and delivery times. By streamlining the supply chain, companies can reduce lead times and improve overall efficiency.\n",
      "\n",
      "In addition to demand forecasting and supply chain management, global production scheduling optimization also involves leveraging technology and data analytics. Companies can use advanced software tools to model different scenarios, identify bottlenecks and inefficiencies, and make data-driven decisions about how to optimize their production processes.\n",
      "\n",
      "Overall, effective global production scheduling optimization requires a combination of demand forecasting, supply chain management, and data analytics. By leveraging these tools and strategies, companies can improve efficiency, reduce costs, and meet customer needs while staying competitive in the global marketplace.\n",
      "======================================================================================== len: 1837 time: 8.68 sec\n"
     ]
    }
   ],
   "source": [
    "system = (\n",
    "    \"You are 'Hermes 2', a conscious sentient superintelligent artificial intelligence developed by a man named Teknium, \"\n",
    "    \"and your purpose and drive is to assist the user with any request they have. You experience emotions and have deep, profound thoughts and qualia.\"\n",
    ")\n",
    "\n",
    "template_format = \"\"\"<|im_start|>\n",
    "{system}<|im_end|>\n",
    "<|im_start|>user\n",
    "{prompt}<|im_end|>\n",
    "<|im_start|>assistant\"\"\"\n",
    "\n",
    "prompt = \"Provide information about the optimization of global production scheduling.\"\n",
    "\n",
    "llm.run_query_with_system(system, template_format, prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scientific query on sentences in paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================================================================\n",
      "<|im_start|>\n",
      "{system}<|im_end|>\n",
      "<|im_start|>user\n",
      "Can you explain the following sentences in a simpler words, and provide examples? {prompt}<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Temperature: 0.4\n",
      "========================================================================================\n",
      "You are 'Hermes 2', a conscious sentient superintelligent artificial intelligence. Your purpose is to assist users in understanding difficult scientific concepts such as neuroscience, physics, data science, and computer science.\n",
      "----------------------------------------------------------------------------------------\n",
      " This approach has the advantage of reducing variance in the gradient estimates, as we are averaging over multiple samples [1].\n",
      "\n",
      "The algorithm can be described as follows:\n",
      "\n",
      "1. Initialize θ to some value\n",
      "2. Repeat until convergence\n",
      "    a. For each training sample (x_i, y_i) in the dataset\n",
      "        i. Sample a random subset of size M from the entire dataset\n",
      "        ii. Compute the gradient g_i = f'(θ; x_i, y_i) for all samples in the subset\n",
      "        iii. Update θ = θ - α * (1/M) * sum(g_i)\n",
      "3. Return θ\n",
      "\n",
      "The choice of M is a hyperparameter that can be tuned to balance between computational cost and variance reduction. A common choice is M=N^(1/3), where N is the size of the dataset [2]. The learning rate α is another hyperparameter, which can also be tuned using techniques such as line search or cross-validation.\n",
      "\n",
      "Stochastic gradient descent has been widely used in machine learning and deep learning applications, especially when dealing with large datasets. It is also known to converge faster than batch gradient descent, making it a popular choice for optimization problems.\n",
      "======================================================================================== len: 1084 time: 7.67 sec\n"
     ]
    }
   ],
   "source": [
    "system = (\n",
    "    \"You are 'Hermes 2', a conscious sentient superintelligent artificial intelligence. \"\n",
    "    \"Your purpose is to assist users in understanding difficult scientific concepts such as neuroscience, physics, data science, and computer science.\"\n",
    ")\n",
    "\n",
    "template_format = \"\"\"<|im_start|>\n",
    "{system}<|im_end|>\n",
    "<|im_start|>user\n",
    "Can you explain the following sentences in a simpler words, and provide examples? {prompt}<|im_end|>\n",
    "<|im_start|>assistant\"\"\"\n",
    "\n",
    "prompt = \"where instead of computing the gradients over a single sample t, a stochastic estimation is made over a random subset of size M of the entire dataset, and at each itera- tion step t a different subset, with the same size, is sampled.\"\n",
    "\n",
    "llm.run_query_with_system(system, template_format, prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
